--- ./update/examples/export-lora/export-lora.cpp	2024-07-09 10:59:17
+++ ../Sources/llmfarm_core_cpp/finetune/export-lora.cpp	2024-07-09 11:21:02
@@ -1,7 +1,8 @@
-
-#include "common.h"
-#include "ggml.h"
-#include "ggml-alloc.h"
+#include "../spm-headers/export-lora.h"
+#include "../ggml/ggml.h"
+#include "../ggml/ggml-alloc.h"
+#include "../ggml/ggml-backend.h"
+#include "../ggml/common.h"
 
 #include <vector>
 #include <string>
@@ -356,7 +357,7 @@
     return true;
 }
 
-static void export_lora(struct export_lora_params * params) {
+static void export_lora(struct export_lora_params * params, bool(*swift_callback)(double)) {
     // load all loras
     std::vector<struct lora_data *> loras;
     for (size_t i = 0; i < params->lora.size(); ++i) {
@@ -368,7 +369,6 @@
     if (loras.size() == 0) {
         fprintf(stderr, "warning: no lora adapters will be applied.\n");
     }
-
     // open input file
     struct llama_file fin(params->fn_model_base.c_str(), "rb");
     if (!fin.fp) {
@@ -436,6 +436,7 @@
         if (i % 2 == 0) {
             printf(".");
         }
+        swift_callback((double)i/(double)n_tensors);
     }
     printf("\n");
 
@@ -447,16 +448,17 @@
     for (size_t i = 0; i < loras.size(); ++i) {
         free_lora(loras[i]);
     }
+    swift_callback(1.0);
 }
 
-int main(int argc, char ** argv) {
+int export_lora_main(int argc, char ** argv, bool(*swift_callback)(double)) {
     struct export_lora_params params = get_default_export_lora_params();
 
     if (!export_lora_params_parse(argc, argv, &params)) {
         return 1;
     }
 
-    export_lora(&params);
+    export_lora(&params,swift_callback);
 
     return 0;
 }
